############## Spark on Cloudera #############


--*** Testing Spark ***---

hdfs dfs -put poems /input

cd /opt/cloudera/parcels/CDH/lib/spark/bin/

spark-shell

val myfile = sc.textFile("hdfs://ip-172-31-0-198.ec2.internal:8020/input/")
val counts = myfile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _)
counts.saveAsTextFile("hdfs://ip-172-31-0-198.ec2.internal:8020/out1/")

pyspark

myfile = sc.textFile("hdfs://ip-172-31-0-198.ec2.internal:8020/input/")
counts = myfile.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).reduceByKey(lambda v1,v2: v1 + v2)
counts.saveAsTextFile("hdfs://ip-172-31-0-198.ec2.internal:8020/out2/")


spark-submit --class org.apache.spark.examples.SparkPi --master yarn \--deploy-mode client /opt/cloudera/parcels/CDH/lib/spark/examples/jars/spark-examples_2.11-2.4.7.7.1.7.0-551.jar 10

spark-submit --class org.apache.spark.examples.SparkPi --master yarn \--deploy-mode cluster /opt/cloudera/parcels/CDH/lib/spark/examples/jars/spark-examples_2.11-2.4.7.7.1.7.0-551.jar 10


--*** Checking Configuration ***--

In spark - search

spark.dynamicAllocation.enabled	






#############################
---**** Hive on Spark ****---
#############################



## Check whether Hive on spark is enabled

In Hive - search - Spark On YARN Service

## Check the default execution engine

In Hive - search - Default Execution Engine

## Connect to Hue

--> Determine the execution engine

set hive.execution.engine;

--> Set it to spark

set hive.execution.engine=spark;

set hive.execution.engine;


## Fire some queries


create table mytable (a int,b string);

insert into table mytable values (1,'abcd');

(check the info logs for spark details )
